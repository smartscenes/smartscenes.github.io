---
title: Projects
layout: page
tags: ['page']
pageOrder: 1
---

We have worked on various projects towards a <em>concrete, grounded, semantic</em> understanding of 3D environments.  What does it mean to understand 3D environments?  Our key insight is to look at real environments as spaces in which human actions take place.  Indoor spaces in particular are canvases that are designed by people to facilitate everyday human actions.  As people spend most of their time indoors, the focus of visual computing research on interiors will become more and more important.

We seek to answer a number of research questions:
- How do people interact with real environments?  What
- What names do people give to objects in scenes?  How are these names dependent on the appearance, functionality, and context?
- What are good representations for 3D scenes?  What features of 3D scene representations of real environments are most salient to people?
- How do environments evolve and change over time?

## Scene understanding and representation

Underlying any attempt at understanding environments, we must consider how environments are to be represented.  Structured representations that allow for manipulation and changes.  We ask whether common knowledge about environments can be learned from data.  What representation do we need that will allow us to generate and synthesis new environments, to allow users to easily manipulate and interact with an virtual environment? 

## Human centric understanding of environments

One important key to understanding environment is to examine how people interact with their surroundings and how the environment is structured and designed to support human activities.  This is especially true of indoor environments, where architects, builders, and designer has specifically created spaces for us to live and work in.  We can view environments as <em>spaces for action</em>.  
- SceneGrok ([webpage](http://graphics.stanford.edu/projects/scenegrok/))
  Where can a person do X?
- Activity based scene synthesis ([webpage](http://graphics.stanford.edu/projects/actsynth/))
  Given that indoor 
- PiGraphs ([webpage](http://graphics.stanford.edu/projects/pigraphs/))

## Scene synthesis
We believe that true understanding goes beyond passive recognition.  That it is important to be able to create new environments.  To that end, we have a series of projects that aims to syntheis new scenes.
- Example based scene synthesis ([webpage](http://graphics.stanford.edu/projects/scenesynth/))   
- Activity based scene synthesis ([webpage](http://graphics.stanford.edu/projects/actsynth/))
- Text to 3D scene generation ([webpage](https://nlp.stanford.edu/data/text2scene.shtml), [demo](https://dovahkiin.stanford.edu/fuzzybox/text2scene.html))
- Scene editing ([demo](https://dovahkiin.stanford.edu/fuzzybox/scene-suggest.html))

## Learning from synthetic data and simulation
Whether we can utilize synthesic scenes to generate training data for learning in a virtual world.
- SSCNet ([webpage](http://vision.princeton.edu/projects/2016/SSCNet/))
- Characterizing Structural Relationships in Scenes using Graph Kernels ([webpage](http://msavva.github.io/files/graphkernel.pdf))
- Physically based rendering ([webpage](http://robots.princeton.edu/projects/2016/PBRS/)])
- Learning where to look ([pdf](https://arxiv.org/abs/1704.02393))




